{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655680f4",
   "metadata": {},
   "source": [
    "# File Ingestion and Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11eefb9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06743f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from pyarrow import csv, parquet\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as PysparkFunc\n",
    "import pyspark.sql.types as PysparkType\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        try:\n",
    "            return yaml.safe_load(file)\n",
    "        except yaml.YAMLError as error:\n",
    "            print(error)\n",
    "            \n",
    "def remove_dir(dirpath):\n",
    "    try:\n",
    "        if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "            shutil.rmtree(dirpath)\n",
    "            print(f'Existing {dirpath} removed.')\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        \n",
    "def normalize_inbound(name):\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r'[\\W_]*','',name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786607c6",
   "metadata": {},
   "source": [
    "## 1. Read file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940657ae",
   "metadata": {},
   "source": [
    "### - Write `config.yml`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eac93",
   "metadata": {},
   "source": [
    "**Note:** *filename:*`bikes-1` is a valid while *filename:*`bikes-2` is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e810212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yml\n",
    "inbound:\n",
    "    folder: data/input/\n",
    "    filename: bikes-1\n",
    "    filetype: csv\n",
    "    header: True\n",
    "    delimiter: ','\n",
    "    skip_rows: 0\n",
    "outbound:\n",
    "    folder: data/output/\n",
    "    filename: bikes-1\n",
    "    filetype: gzip\n",
    "    header: True\n",
    "    delimiter: '|'\n",
    "columns:\n",
    "    - bike_id: int\n",
    "    - start_time: timestamp\n",
    "    - end_time: timestamp   \n",
    "    - start_station_name: string\n",
    "    - end_station_name: string\n",
    "column_name_splitter: '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7690cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_name_splitter': '_',\n",
      " 'columns': [{'bike_id': 'int'},\n",
      "             {'start_time': 'timestamp'},\n",
      "             {'end_time': 'timestamp'},\n",
      "             {'start_station_name': 'string'},\n",
      "             {'end_station_name': 'string'}],\n",
      " 'inbound': {'delimiter': ',',\n",
      "             'filename': 'bikes-1',\n",
      "             'filetype': 'csv',\n",
      "             'folder': 'data/input/',\n",
      "             'header': True,\n",
      "             'skip_rows': 0},\n",
      " 'outbound': {'delimiter': '|',\n",
      "              'filename': 'bikes-1',\n",
      "              'filetype': 'gzip',\n",
      "              'folder': 'data/output/',\n",
      "              'header': True}}\n"
     ]
    }
   ],
   "source": [
    "config = read_config_file('config.yml')\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26ee6b",
   "metadata": {},
   "source": [
    "### - Convert `csv` file into `parquet` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c38b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder': 'data/input/',\n",
       " 'filename': 'bikes-1',\n",
       " 'filetype': 'csv',\n",
       " 'header': True,\n",
       " 'delimiter': ',',\n",
       " 'skip_rows': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['inbound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc52a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing data/input/bikes-1.parquet removed.\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dask dataframe\n",
    "# create parquet file\n",
    "inboundfile = config['inbound']['folder']+config['inbound']['filename']+'.'+config['inbound']['filetype']\n",
    "incolnames = []\n",
    "parquetfile = config['inbound']['folder']+config['inbound']['filename']+'.parquet'\n",
    "# remove parquetfile if exists\n",
    "remove_dir(parquetfile)\n",
    "df = None\n",
    "if config['inbound']['filetype'] == 'csv':\n",
    "    df = dd.read_csv(inboundfile, \n",
    "                     delimiter=config['inbound']['delimiter'],\n",
    "                     header='infer' if config['inbound']['header'] else None,\n",
    "                     skiprows=config['inbound']['skip_rows'] if config['inbound']['skip_rows'] else None,\n",
    "                     assume_missing=True)\n",
    "    df.to_parquet(parquetfile) # time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0d38c",
   "metadata": {},
   "source": [
    "### - Load `parquet` file into `pyspark` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f106ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FileIngestion')\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89715962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rental_id: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- bike_id: double (nullable = true)\n",
      " |-- end_rental_date_time: string (nullable = true)\n",
      " |-- end_station_id: double (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- start_rental_date_time: string (nullable = true)\n",
      " |-- start_station_id: double (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- __null_dask_index__: long (nullable = true)\n",
      "\n",
      "+-----------+--------+-------+--------------------+--------------+--------------------+----------------------+----------------+--------------------+-------------------+\n",
      "|  rental_id|duration|bike_id|end_rental_date_time|end_station_id|    end_station_name|start_rental_date_time|start_station_id|  start_station_name|__null_dask_index__|\n",
      "+-----------+--------+-------+--------------------+--------------+--------------------+----------------------+----------------+--------------------+-------------------+\n",
      "|7.2091291E7|   480.0|12256.0| 2017-12-15 11:20:00|          88.0|Bayley Street , B...|   2017-12-15 11:12:00|           793.0|Cromer Street, Bl...|                  0|\n",
      "|7.2091299E7|   600.0|13632.0| 2017-12-15 11:22:00|         448.0|Fisherman's Walk ...|   2017-12-15 11:12:00|           483.0|Albert Gardens, S...|                  1|\n",
      "|7.2091282E7|   360.0| 5478.0| 2017-12-15 11:18:00|         280.0|Royal Avenue 2, C...|   2017-12-15 11:12:00|           649.0|World's End Place...|                  2|\n",
      "|7.2091295E7|   300.0|13262.0| 2017-12-15 11:17:00|         173.0|Waterloo Road, So...|   2017-12-15 11:12:00|           810.0|Tate Modern, Bank...|                  3|\n",
      "|7.2091286E7|   780.0| 9027.0| 2017-12-15 11:25:00|         197.0|Stamford Street, ...|   2017-12-15 11:12:00|           135.0|Clerkenwell Green...|                  4|\n",
      "|7.2091287E7|  1080.0|11446.0| 2017-12-15 11:30:00|         727.0|Chesilton Road, F...|   2017-12-15 11:12:00|           350.0|Queen's Gate, Ken...|                  5|\n",
      "|7.2091284E7|   240.0|14507.0| 2017-12-15 11:16:00|         483.0|Albert Gardens, S...|   2017-12-15 11:12:00|           282.0|Royal London Hosp...|                  6|\n",
      "|7.2091293E7|  2040.0|12555.0| 2017-12-15 11:46:00|         591.0|Westfield Library...|   2017-12-15 11:12:00|           242.0|Beaumont Street, ...|                  7|\n",
      "|7.2091289E7|   600.0| 7629.0| 2017-12-15 11:22:00|         104.0|    Crosswall, Tower|   2017-12-15 11:12:00|           552.0|Watney Street, Sh...|                  8|\n",
      "| 7.209131E7|   240.0| 4488.0| 2017-12-15 11:17:00|         108.0|Abbey Orchard Str...|   2017-12-15 11:13:00|           267.0|Regency Street, W...|                  9|\n",
      "+-----------+--------+-------+--------------------+--------------+--------------------+----------------------+----------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.format('parquet').options(header=config['inbound']['header'], \n",
    "                                          inferSchema='True').load(parquetfile) \n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cdd91",
   "metadata": {},
   "source": [
    "## 2. Schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9970bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_station_name matches end_station_name.\n",
      "end_station_name of dtype string matches dtype string\n",
      "\n",
      "start_rental_date_time matches start_time.\n",
      "start_rental_date_time of dtype string doesn't match dtype timestamp\n",
      "\n",
      "bike_id matches bike_id.\n",
      "bike_id of dtype double doesn't match dtype int\n",
      "\n",
      "start_station_name matches start_station_name.\n",
      "start_station_name of dtype string matches dtype string\n",
      "\n",
      "end_rental_date_time matches end_time.\n",
      "end_rental_date_time of dtype string doesn't match dtype timestamp\n",
      "\n",
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validincolnames = []\n",
    "validconfigcolnames = []\n",
    "incolumns = list(set(df.dtypes))\n",
    "incolnames = [name for name, dtype in incolumns]\n",
    "configcolnames = [name for col in config['columns'] for name, dtype in col.items()]\n",
    "\n",
    "for incol in incolumns:\n",
    "    incolname = incol[0]\n",
    "    incoldtype = incol[1]\n",
    "    for configcol in config['columns']:\n",
    "        for configcolname, configcoldtype in configcol.items():\n",
    "            # validate incolname as per configcolname\n",
    "            configcolname = configcolname.strip().lower()\n",
    "            allparts = configcolname.split(config['column_name_splitter'])\n",
    "            partsfound = 0\n",
    "            for part in allparts:\n",
    "                if part in normalize_inbound(incolname):\n",
    "                    partsfound += 1\n",
    "            if partsfound == len(allparts):\n",
    "                validincolnames += [incolname]\n",
    "                validconfigcolnames += [configcolname]\n",
    "                print(f'{incolname} matches {configcolname}.')\n",
    "                # validate incoldtype as per configcoldtype\n",
    "                configcoldtype = configcoldtype.strip().lower()\n",
    "                if configcoldtype in normalize_inbound(incoldtype):\n",
    "                    print(f'{incolname} of dtype {incoldtype} matches dtype {configcoldtype}\\n')\n",
    "                else:\n",
    "                    print(f'{incolname} of dtype {incoldtype} doesn\\'t match dtype {configcoldtype}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c06aa",
   "metadata": {},
   "source": [
    "## 3. Validation result and decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c35ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation passed and file accepted.\n",
      "Writing outbound compressed file ... Done\n",
      "Reading ingested file ... Done\n",
      "{'file_size_bytes': 810570730,\n",
      " 'filename': 'data\\\\output\\\\bikes-1\\\\part-00000-de65e3a6-0d58-4989-9884-19fdd99a13cd-c000.csv.gz',\n",
      " 'num_cols': 7,\n",
      " 'num_rows': 38215560}\n",
      "Following INBOUND columns are not in CONFIG ['duration', '__null_dask_index__', 'end_station_id', 'rental_id', 'start_station_id']\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if len(validincolnames) == len(config['columns']):\n",
    "    print('Schema validation passed and file accepted.')\n",
    "    # rename INBOUND column names as per CONFIG column names\n",
    "    colargs = []\n",
    "    for i, validincolname in enumerate(validincolnames):\n",
    "        colargs += [PysparkFunc.col(validincolname).alias(validconfigcolnames[i])]\n",
    "    valid_df = df.select(*colargs)\n",
    "    # add increasing index column\n",
    "    valid_df = valid_df.withColumn('index', PysparkFunc.monotonically_increasing_id())\n",
    "    # add inbound file path column\n",
    "    valid_df = valid_df.withColumn('inbound_file', PysparkFunc.lit(inboundfile))\n",
    "    # write outbound compressed file\n",
    "    outboundfile = config['outbound']['folder']+config['outbound']['filename']\n",
    "    # remove if exists\n",
    "    remove_dir(outboundfile)\n",
    "    print('Writing outbound compressed file ...',end=' ')\n",
    "    valid_df.repartition(1).write.options(header=config['outbound']['header'],\n",
    "                              delimiter=config['outbound']['delimiter'],\n",
    "                              compression=config['outbound']['filetype']).csv(outboundfile)\n",
    "    print('Done')\n",
    "    # read ingested file\n",
    "    print('Reading ingested file ...',end=' ')\n",
    "    df = spark.read.format('csv').options(header=config['outbound']['header'],\n",
    "                                          delimiter=config['outbound']['delimiter'],\n",
    "                                          inferSchema='True').load(outboundfile)\n",
    "    print('Done')\n",
    "    # get rows count\n",
    "    num_rows = df.count()\n",
    "    # get columns count\n",
    "    num_cols = len(df.columns)\n",
    "    # get input file size\n",
    "    inputfile = df.select(PysparkFunc.input_file_name()).first().__getitem__('input_file_name()').strip('file:///')\n",
    "    inputfile = inputfile.replace('%20',' ').replace('/','\\\\').replace(os.getcwd()+'\\\\','')\n",
    "    file_size = os.path.getsize(inputfile)\n",
    "    # create summary file\n",
    "    summary = {\n",
    "        'filename':inputfile,\n",
    "        'num_cols':num_cols,\n",
    "        'num_rows':num_rows,\n",
    "        'file_size_bytes':file_size\n",
    "    }\n",
    "    with open(config['outbound']['folder']+config['outbound']['filename']+'.yml','w') as file:\n",
    "        yaml.dump(summary,file)\n",
    "    pprint(summary)\n",
    "else:\n",
    "    print('Schema validation failed and file rejected.')\n",
    "    print(f'Following CONFIG columns are not in INBOUND {list(set(configcolnames).difference(set(validconfigcolnames)))}')\n",
    "\n",
    "if len(validincolnames) != len(incolnames):\n",
    "    print(f'Following INBOUND columns are not in CONFIG {list(set(incolnames).difference(set(validincolnames)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7499d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae158d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
