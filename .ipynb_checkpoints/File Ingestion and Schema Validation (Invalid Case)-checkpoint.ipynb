{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655680f4",
   "metadata": {},
   "source": [
    "# File Ingestion and Schema Validation (Invalid Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11eefb9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06743f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from pyarrow import csv, parquet\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as PysparkFunc\n",
    "import pyspark.sql.types as PysparkType\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        try:\n",
    "            return yaml.safe_load(file)\n",
    "        except yaml.YAMLError as error:\n",
    "            print(error)\n",
    "            \n",
    "def remove_dir(dirpath):\n",
    "    try:\n",
    "        if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
    "            shutil.rmtree(dirpath)\n",
    "            print(f'Existing {dirpath} removed.')\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        \n",
    "def normalize_inbound(name):\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r'[\\W_]*','',name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786607c6",
   "metadata": {},
   "source": [
    "## 1. Read file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940657ae",
   "metadata": {},
   "source": [
    "### - Write `config.yml`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eac93",
   "metadata": {},
   "source": [
    "**Note:** *filename:*`bikes-1` is a valid while *filename:*`bikes-2` is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e810212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yml\n",
    "inbound:\n",
    "    folder: data/input/\n",
    "    filename: bikes-2\n",
    "    filetype: csv\n",
    "    header: True\n",
    "    delimiter: ','\n",
    "    skip_rows: 0\n",
    "outbound:\n",
    "    folder: data/output/\n",
    "    filename: bikes-2\n",
    "    filetype: gzip\n",
    "    header: True\n",
    "    delimiter: '|'\n",
    "columns:\n",
    "    - bike_id: int\n",
    "    - start_time: timestamp\n",
    "    - end_time: timestamp   \n",
    "    - start_station_name: string\n",
    "    - end_station_name: string\n",
    "column_name_splitter: '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7690cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_name_splitter': '_',\n",
      " 'columns': [{'bike_id': 'int'},\n",
      "             {'start_time': 'timestamp'},\n",
      "             {'end_time': 'timestamp'},\n",
      "             {'start_station_name': 'string'},\n",
      "             {'end_station_name': 'string'}],\n",
      " 'inbound': {'delimiter': ',',\n",
      "             'filename': 'bikes-2',\n",
      "             'filetype': 'csv',\n",
      "             'folder': 'data/input/',\n",
      "             'header': True,\n",
      "             'skip_rows': 0},\n",
      " 'outbound': {'delimiter': '|',\n",
      "              'filename': 'bikes-2',\n",
      "              'filetype': 'gzip',\n",
      "              'folder': 'data/output/',\n",
      "              'header': True}}\n"
     ]
    }
   ],
   "source": [
    "config = read_config_file('config.yml')\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26ee6b",
   "metadata": {},
   "source": [
    "### - Convert `csv` file into `parquet` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c38b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder': 'data/input/',\n",
       " 'filename': 'bikes-2',\n",
       " 'filetype': 'csv',\n",
       " 'header': True,\n",
       " 'delimiter': ',',\n",
       " 'skip_rows': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['inbound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc52a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing data/input/bikes-2.parquet removed.\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dask dataframe\n",
    "# create parquet file\n",
    "inboundfile = config['inbound']['folder']+config['inbound']['filename']+'.'+config['inbound']['filetype']\n",
    "incolnames = []\n",
    "parquetfile = config['inbound']['folder']+config['inbound']['filename']+'.parquet'\n",
    "# remove parquetfile if exists\n",
    "remove_dir(parquetfile)\n",
    "df = None\n",
    "if config['inbound']['filetype'] == 'csv':\n",
    "    df = dd.read_csv(inboundfile, \n",
    "                     delimiter=config['inbound']['delimiter'],\n",
    "                     header='infer' if config['inbound']['header'] else None,\n",
    "                     skiprows=config['inbound']['skip_rows'] if config['inbound']['skip_rows'] else None,\n",
    "                     assume_missing=True)\n",
    "    df.to_parquet(parquetfile) # time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0d38c",
   "metadata": {},
   "source": [
    "### - Load `parquet` file into `pyspark` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f106ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('FileIngestion')\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89715962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip Duration: double (nullable = true)\n",
      " |-- Start Time: string (nullable = true)\n",
      " |-- Stop Time: string (nullable = true)\n",
      " |-- Start Station ID: double (nullable = true)\n",
      " |-- End Station ID: double (nullable = true)\n",
      " |-- Bike ID: double (nullable = true)\n",
      " |-- User Type: double (nullable = true)\n",
      " |-- Birth Year: double (nullable = true)\n",
      " |-- Gender: double (nullable = true)\n",
      " |-- __null_dask_index__: long (nullable = true)\n",
      "\n",
      "+-------------+--------------------+--------------------+----------------+--------------+-------+---------+----------+------+-------------------+\n",
      "|Trip Duration|          Start Time|           Stop Time|Start Station ID|End Station ID|Bike ID|User Type|Birth Year|Gender|__null_dask_index__|\n",
      "+-------------+--------------------+--------------------+----------------+--------------+-------+---------+----------+------+-------------------+\n",
      "|        399.0|2018-01-20 16:05:...|2018-01-20 16:12:...|          3602.0|        3570.0|32886.0|      1.0|    1976.0|   2.0|                  0|\n",
      "|        257.0|2018-01-20 16:08:...|2018-01-20 16:13:...|          3602.0|        3598.0|14781.0|      1.0|    1979.0|   1.0|                  1|\n",
      "|        675.0|2018-01-20 17:01:...|2018-01-20 17:12:...|          3602.0|        3605.0|19795.0|      1.0|    1978.0|   2.0|                  2|\n",
      "|        169.0|2018-01-20 17:08:...|2018-01-20 17:10:...|          3602.0|        3603.0|32138.0|      1.0|    1979.0|   1.0|                  3|\n",
      "|       3132.0|2018-01-20 18:47:...|2018-01-20 19:39:...|          3602.0|         527.0|31471.0|      1.0|    1988.0|   1.0|                  4|\n",
      "|        579.0|2018-01-21 00:10:...|2018-01-21 00:20:...|          3602.0|        3588.0|33453.0|      1.0|    1990.0|   1.0|                  5|\n",
      "|        134.0|2018-01-21 09:30:...|2018-01-21 09:32:...|          3602.0|        3595.0|32923.0|      1.0|    1982.0|   1.0|                  6|\n",
      "|        280.0|2018-01-21 10:41:...|2018-01-21 10:46:...|          3602.0|        3568.0|27297.0|      1.0|    1989.0|   2.0|                  7|\n",
      "|        430.0|2018-01-21 13:25:...|2018-01-21 13:32:...|          3602.0|        3617.0|30977.0|      1.0|    1977.0|   1.0|                  8|\n",
      "|       6336.0|2018-01-21 14:05:...|2018-01-21 15:50:...|          3602.0|        3595.0|30314.0|      0.0|    1969.0|   0.0|                  9|\n",
      "+-------------+--------------------+--------------------+----------------+--------------+-------+---------+----------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.format('parquet').options(header=config['inbound']['header'], \n",
    "                                          inferSchema='True').load(parquetfile) \n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cdd91",
   "metadata": {},
   "source": [
    "## 2. Schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9970bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time matches start_time.\n",
      "Start Time of dtype string doesn't match dtype timestamp\n",
      "\n",
      "Bike ID matches bike_id.\n",
      "Bike ID of dtype double doesn't match dtype int\n",
      "\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "validincolnames = []\n",
    "validconfigcolnames = []\n",
    "incolumns = list(set(df.dtypes))\n",
    "incolnames = [name for name, dtype in incolumns]\n",
    "configcolnames = [name for col in config['columns'] for name, dtype in col.items()]\n",
    "\n",
    "for incol in incolumns:\n",
    "    incolname = incol[0]\n",
    "    incoldtype = incol[1]\n",
    "    for configcol in config['columns']:\n",
    "        for configcolname, configcoldtype in configcol.items():\n",
    "            # validate incolname as per configcolname\n",
    "            configcolname = configcolname.strip().lower()\n",
    "            allparts = configcolname.split(config['column_name_splitter'])\n",
    "            partsfound = 0\n",
    "            for part in allparts:\n",
    "                if part in normalize_inbound(incolname):\n",
    "                    partsfound += 1\n",
    "            if partsfound == len(allparts):\n",
    "                validincolnames += [incolname]\n",
    "                validconfigcolnames += [configcolname]\n",
    "                print(f'{incolname} matches {configcolname}.')\n",
    "                # validate incoldtype as per configcoldtype\n",
    "                configcoldtype = configcoldtype.strip().lower()\n",
    "                if configcoldtype in normalize_inbound(incoldtype):\n",
    "                    print(f'{incolname} of dtype {incoldtype} matches dtype {configcoldtype}\\n')\n",
    "                else:\n",
    "                    print(f'{incolname} of dtype {incoldtype} doesn\\'t match dtype {configcoldtype}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c06aa",
   "metadata": {},
   "source": [
    "## 3. Validation result and decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c35ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation failed and file rejected.\n",
      "Following CONFIG columns are not in INBOUND ['end_time', 'end_station_name', 'start_station_name']\n",
      "Following INBOUND columns are not in CONFIG ['End Station ID', 'User Type', 'Stop Time', 'Start Station ID', 'Trip Duration', 'Gender', 'Birth Year', '__null_dask_index__']\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if len(validincolnames) == len(config['columns']):\n",
    "    print('Schema validation passed and file accepted.')\n",
    "    # rename INBOUND column names as per CONFIG column names\n",
    "    colargs = []\n",
    "    for i, validincolname in enumerate(validincolnames):\n",
    "        colargs += [PysparkFunc.col(validincolname).alias(validconfigcolnames[i])]\n",
    "    valid_df = df.select(*colargs)\n",
    "    # add increasing index column\n",
    "    valid_df = valid_df.withColumn('index', PysparkFunc.monotonically_increasing_id())\n",
    "    # add inbound file path column\n",
    "    valid_df = valid_df.withColumn('inbound_file', PysparkFunc.lit(inboundfile))\n",
    "    # write outbound compressed file\n",
    "    outboundfile = config['outbound']['folder']+config['outbound']['filename']\n",
    "    # remove if exists\n",
    "    remove_dir(outboundfile)\n",
    "    print('Writing outbound compressed file ...',end=' ')\n",
    "    valid_df.repartition(1).write.options(header=config['outbound']['header'],\n",
    "                              delimiter=config['outbound']['delimiter'],\n",
    "                              compression=config['outbound']['filetype']).csv(outboundfile)\n",
    "    print('Done')\n",
    "    # read ingested file\n",
    "    print('Reading ingested file ...',end=' ')\n",
    "    df = spark.read.format('csv').options(header=config['outbound']['header'],\n",
    "                                          delimiter=config['outbound']['delimiter'],\n",
    "                                          inferSchema='True').load(outboundfile)\n",
    "    print('Done')\n",
    "    # get rows count\n",
    "    num_rows = df.count()\n",
    "    # get columns count\n",
    "    num_cols = len(df.columns)\n",
    "    # get input file size\n",
    "    inputfile = df.select(PysparkFunc.input_file_name()).first().__getitem__('input_file_name()').strip('file:///')\n",
    "    inputfile = inputfile.replace('%20',' ').replace('/','\\\\').replace(os.getcwd()+'\\\\','')\n",
    "    file_size = os.path.getsize(inputfile)\n",
    "    # create summary file\n",
    "    summary = {\n",
    "        'filename':inputfile,\n",
    "        'num_cols':num_cols,\n",
    "        'num_rows':num_rows,\n",
    "        'file_size_bytes':file_size\n",
    "    }\n",
    "    with open(config['outbound']['folder']+config['outbound']['filename']+'.yml','w') as file:\n",
    "        yaml.dump(summary,file)\n",
    "    pprint(summary)\n",
    "else:\n",
    "    print('Schema validation failed and file rejected.')\n",
    "    print(f'Following CONFIG columns are not in INBOUND {list(set(configcolnames).difference(set(validconfigcolnames)))}')\n",
    "\n",
    "if len(validincolnames) != len(incolnames):\n",
    "    print(f'Following INBOUND columns are not in CONFIG {list(set(incolnames).difference(set(validincolnames)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7499d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae158d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
